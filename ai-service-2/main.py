"""
Updated main.py
Hybrid Rule-based + Embeddings-RAG + Safe-SQL engine
Assumptions:
 - You have a working `rag_search.py` with function `retrieve_relevant_chunks(query, top_k=5)`
   which loads prebuilt embeddings index (rag_index.npz) and returns a list of text chunks.
 - `ollama` binary is installed and `llama3` model is available.
 - Keep your existing folder layout: data/, models/, rag/, etc.
 - This file preserves original endpoints: /ping, /latest, /all, /range, /forecast, /analytics, /ask
"""

import os
import re
import json
import logging
import shutil
import subprocess
from datetime import datetime, timedelta

import joblib
import numpy as np
import pandas as pd

from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
import sqlite3

from passlib.context import CryptContext
from fastapi import HTTPException
from pydantic import BaseModel

class ChatRequest(BaseModel):
    conversation_id: str
    message: str

class ChatResponse(BaseModel):
    content: str


from rag_search import retrieve_relevant_chunks

 

BASE_DIR = os.path.dirname(os.path.abspath(__file__))

MODEL_DIR = os.path.join(BASE_DIR, "models")

# Revenue model will be loaded lazily inside endpoints to avoid startup crashes
REVENUE_MODEL = None



logging.basicConfig(level=logging.INFO)
logging.info("üöÄ FASTAPI starting...")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
BASE_DIR = os.path.dirname(__file__)
DB_PATH = os.path.join(BASE_DIR, "data", "meter.db")
pwd_context = CryptContext(
    schemes=["pbkdf2_sha256"],
    deprecated="auto"
)

RAG_DIR = os.path.join(BASE_DIR, "rag")

LLM_MODEL = "deepseek-r1"


# model runner
LLM_RUN_CMD = ["ollama", "run", LLM_MODEL]

logging.info(f"ü§ñ Using LLM model: {LLM_MODEL}")



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ RAG / EMBEDDINGS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
# This file expects a separate module `rag_search.py` with function:
#    retrieve_relevant_chunks(query: str, top_k: int = 5) -> List[str]
#
# If you don't have that module, create it (we kept separation of concerns).
try:
    from rag_search import retrieve_relevant_chunks
except Exception:
    retrieve_relevant_chunks = None
    logging.warning("rag_search.retrieve_relevant_chunks NOT available. /ask fallback will error if used.")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SAFETY / TEMPLATING HELPERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

FORBIDDEN_SQL_PATTERNS = [
    r";", r"\bDROP\b", r"\bDELETE\b", r"\bINSERT\b", r"\bUPDATE\b",
    r"\bALTER\b", r"\bATTACH\b", r"\bDETACH\b", r"\bPRAGMA\b", r"\bVACUUM\b",
    r"\bEXECUTE\b", r"\bREPLACE\b", r"\bCREATE\b", r"\bTRUNCATE\b"
]


def safe_sql_check(sql: str) -> (bool, str):
    """Return (ok, message). Simple safety rules enforced for SQL generated by LLM."""
    if not sql or not sql.strip():
        return False, "Empty SQL"
    s = sql.strip()
    # only single statement
    if ";" in s:
        return False, "Unsafe SQL blocked ‚Äî no semicolons allowed"
    # must start with SELECT
    if not re.match(r"^\s*SELECT\b", s, flags=re.I):
        return False, "Unsafe SQL blocked ‚Äî must start with SELECT"
    # forbid dangerous keywords
    for patt in FORBIDDEN_SQL_PATTERNS:
        if re.search(patt, s, flags=re.I):
            return False, f"Unsafe SQL blocked ‚Äî forbidden pattern detected"
    # query length guard
    if len(s) > 8000:
        return False, "Unsafe SQL blocked ‚Äî query too long"
    return True, "OK"


def require_admin(role: str):
    if role != "admin":
        raise HTTPException(
            status_code=403,
            detail="Admin access required for revenue forecasting"
        )


def auto_repair_sql(sql: str) -> str:
    """Small rule-based fixes for common LLM mistakes"""
    fixes = [
        ("meter_readings.feeder_id", "meters.feeder_id"),
        ("FROM meter_readings mr ON", "FROM meter_readings mr JOIN"),
        ("WHERE ts LIKE '__-__-__'", ""),
        ("m.load_kw", "r.load_kw"),
        ("SELECT Show", "SELECT"),  # some LLMs echo the question
    ]

    fixes.extend([
        ("SELECT Show", "SELECT"),
        ("SELECT select", "SELECT"),
        ("FROM meter_readings WHERE WHERE", "FROM meter_readings WHERE"),
        ("GROUP BY GROUP BY", "GROUP BY"),
        ("SELECT SELECT", "SELECT"),
        ("SELECT\nSELECT", "SELECT"),
        ("SQL:", ""), 
        ("Query:", ""), 
        ("Result:", ""), 
        ("Here is the SQL you requested:", ""),
    ])


    out = sql
    for a, b in fixes:
        out = out.replace(a, b)
    return out.strip()


def extract_json_from_text(raw: str):
    """Try to extract JSON object from LLM output (analytics task). Returns dict or None."""
    # Strip <think> tags first
    raw = re.sub(r"<think>[\s\S]*?</think>", "", raw, flags=re.I).strip()
    
    if raw.startswith("{") and raw.endswith("}"):
        try:
            return json.loads(raw)
        except Exception:
            pass
    # try find first { ... } block
    m = re.search(r"\{.*\}", raw, flags=re.S)
    if m:
        try:
            return json.loads(m.group(0))
        except Exception:
            return None
    return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ RULE-BASED INTENT DETECTION & SQL TEMPLATES ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def extract_zone(q: str):
    m = re.search(r"\b(north|south|east|west|central)\b\s+zone", q, flags=re.I)
    return m.group(1).capitalize() if m else None

def detect_intent_and_params(question: str) -> dict:
    """
    Quick rule-based intent detection. Returns:
      {'intent': 'forecast'|'analytics'|'sql_template'|'fallback', 'params': {...}}
    """

    q = (question or "").strip()
    q_low = q.lower()

    # --------------- GREETINGS / GENERIC -------------------
    if re.match(r"^(hi|hello|hey|greetings|hola)\b", q_low):
        return {"intent": "fallback", "params": {}} # Let LLM handle it, but it's matched

    if re.search(r"\b(who are you|what can you do|help|how to use|commands)\b", q_low):
        return {"intent": "fallback", "params": {}}

    # --------------- FORECAST -------------------
    if re.search(r"\b(forecast|predict|tomorrow)\b", q_low) and re.search(r"\bmeter\b|\bmtr\d{3}\b", q_low):
        m = re.search(r"(\d+)\s*hour", q_low)
        hours = int(m.group(1)) if m else 24

        meter_m = re.search(r"meter\s*([A-Za-z0-9]+)", q, flags=re.I)
        meter_id = meter_m.group(1).upper() if meter_m else "MTR001"

        return {"intent": "forecast", "params": {"meter_id": meter_id, "hours": hours}}

    # --------------- ANALYTICS -------------------
    if re.search(r"\b(temperature|temp)\b.*\b(affect|influence|correlat)", q_low):
        return {
            "intent": "analytics",
            "task": "correlation_weather_load",
            "params": {"zone": extract_zone(q), "days": 30},
        }

    if re.search(r"\b(humidity)\b.*\b(affect|influence|correlat)", q_low):
        return {
            "intent": "analytics",
            "task": "correlation_humidity_load",
            "params": {"zone": extract_zone(q), "days": 30},
        }

    if re.search(r"\b(seasonal|seasonality|monthly|seasonal pattern|monthly average)\b", q_low):
        m = re.search(r"meter\s*([A-Za-z0-9]+)", q, flags=re.I)
        meter = m.group(1).upper() if m else None
        return {
            "intent": "analytics",
            "task": "seasonal_pattern",
            "params": {"meter_id": meter, "days": 365},
        }

    if re.search(r"\b(estimate|bill|billing|tariff)\b", q_low) and re.search(r"meter\s*[A-Za-z0-9]+", q_low):
        m = re.search(r"meter\s*([A-Za-z0-9]+)", q, flags=re.I)
        meter = m.group(1).upper() if m else None
        return {
            "intent": "analytics",
            "task": "billing_estimation",
            "params": {"meter_id": meter, "days": 30},
        }

    if re.search(r"\b(anomaly|anomalies|anomalous|outlier|outliers)\b", q_low):
        return {
            "intent": "analytics",
            "task": "anomaly_detection",
            "params": {"zone": extract_zone(q), "days": 30},
        }

    if re.search(r"\b(peak cause|what caused peak|why did peak)\b", q_low):
        return {
            "intent": "analytics",
            "task": "peak_cause_analysis",
            "params": {"zone": extract_zone(q), "days": 7},
        }

    # --------------- SQL TEMPLATE DETECTION -------------------

    # Last N hours/days for meter
    m = re.search(r"last\s+(\d+)\s*(hour|hours|day|days)\b.*meter\s*([A-Za-z0-9]+)", q_low)
    if m:
        n = int(m.group(1))
        unit = m.group(2)
        meter = m.group(3).upper()
        if "hour" in unit:
            return {"intent": "sql_template",
                    "params": {"template": "meter_last_hours", "meter_id": meter, "hours": n}}
        return {"intent": "sql_template",
                "params": {"template": "meter_last_days", "meter_id": meter, "days": n}}

    # Latest load
    m = re.search(r"(latest|last)\s+load.*meter\s*([A-Za-z0-9]+)", q_low)
    if m:
        return {
            "intent": "sql_template",
            "params": {"template": "meter_latest", "meter_id": m.group(2).upper()},
        }

    # Top N peaks this month
    m = re.search(r"top\s*(\d+)\s*peak.*meter\s*([A-Za-z0-9]+)", q_low)
    if m:
        return {
            "intent": "sql_template",
            "params": {
                "template": "top_n_peaks_month",
                "meter_id": m.group(2).upper(),
                "n": int(m.group(1)),
            },
        }

    # Which zone had highest load
    if re.search(r"which zone had the highest|highest load.*zone", q_low):
        if "yesterday" in q_low:
            period = "yesterday"
        elif "week" in q_low:
            period = "week"
        else:
            period = "month"
        return {"intent": "sql_template", "params": {"template": "zone_highest_load", "period": period}}

    # Average load in North zone this week
    m = re.search(r"average load.*in\s+([A-Za-z]+)\s+zone.*this week", q_low)
    if m:
        return {
            "intent": "sql_template",
            "params": {"template": "zone_avg_week", "zone": m.group(1).capitalize()},
        }

    # Feeder highest peak yesterday
    if re.search(r"(feeder).*highest.*peak.*yesterday", q_low):
        f = re.search(r"feeder\s*([A-Za-z0-9]+)", q, flags=re.I)
        feeder_id = f.group(1).upper() if f else None
        return {
            "intent": "sql_template",
            "params": {"template": "feeder_highest_peak_yesterday", "feeder_id": feeder_id},
        }

    # Zone hourly trend
    m = re.search(r"hourly.*(load)?.*trend.*([A-Za-z]+)\s+zone.*last\s+(\d+)\s*days", q_low)
    if m:
        return {
            "intent": "sql_template",
            "params": {
                "template": "zone_hourly_trend_days",
                "zone": m.group(2).capitalize(),
                "days": int(m.group(3)),
            },
        }

    # Top N highest timestamps for meter
    m = re.search(r"top\s*(\d+)\s*highest.*timestamp.*meter\s*([A-Za-z0-9]+)", q_low)
    if m:
        return {
            "intent": "sql_template",
            "params": {
                "template": "meter_top_n_timestamps_month",
                "meter_id": m.group(2).upper(),
                "n": int(m.group(1)),
            },
        }

    # Zone anomaly detection explicitly
    if re.search(r"detect.*anomal", q_low) and "zone" in q_low:
        return {
            "intent": "analytics",
            "task": "zone_anomaly_detection",
            "params": {"zone": extract_zone(q), "days": 30},
        }
    
    # ------------------- REVENUE FORECAST (STRONG NLP MATCHING) -------------------
    # Examples: 
    # "estimate revenue for 2028"
    # "predict income for 2030"
    # "project sales in 2026"
    # "expected revenue 2025"
    # "what will be the revenue in 2032"
    # "revenue forecast next year"
    # "income projection for 2040"

    # if re.search(r"(revenue|income|sales)\s*(forecast|predict|projection|estimate|expected)?", q_low):

    #     # Try to extract a year: any 4-digit year starting with 20xx
    #     year_match = re.search(r"(20\d{2})", q)
    #     year = int(year_match.group(1)) if year_match else None

    #     # Handle queries like "next year", "coming year"
    #     if not year:
    #         if "next year" in q_low:
    #             year = datetime.now().year + 1
    #         elif "this year" in q_low:
    #             year = datetime.now().year
    #         elif "coming year" in q_low:
    #             year = datetime.now().year + 1

    #     return {
    #         "intent": "revenue_forecast",
    #         "params": {
    #             "year": year
    #         }
    #     }

    # REVENUE FORECAST detection (FIXED)
    # if "revenue" in q_low or "income" in q_low or "sales" in q_low:
    #     if "forecast" in q_low or "predict" in q_low or "estimate" in q_low:
            
    #         # Extract year if present
    #         y = re.search(r"(20\d{2})", q)
    #         year = int(y.group(1)) if y else None

    #         # Handle "next year" or "this year"
    #         if not year:
    #             if "next year" in q_low:
    #                 year = datetime.now().year + 1
    #             elif "this year" in q_low:
    #                 year = datetime.now().year
    #             elif "coming year" in q_low:
    #                 year = datetime.now().year + 1

    #         return {
    #             "intent": "revenue_forecast",
    #             "params": {"year": year}
    #         }

    # --- Revenue RANGE detection: ‚Äúfrom 2025 to 2040‚Äù, ‚Äú2025-2035‚Äù, etc ---

    m_range = re.search(r"(20\d{2})\D+(20\d{2})", q)
    if m_range:
        y1 = int(m_range.group(1))
        y2 = int(m_range.group(2))
        if y2 > y1:
            return {
                "intent": "revenue_range_forecast",
                "params": {"start_year": y1, "end_year": y2}
            }

    # --- Revenue SINGLE-YEAR detection ---
    m_year = re.search(r"(20\d{2})", q)
    if m_year:
        return {
            "intent": "revenue_forecast",
            "params": {"year": int(m_year.group(1))}
        }

    # Phrases like ‚Äúnext year / coming year‚Äù
    if "next year" in q_low:
        return {"intent": "revenue_forecast",
                "params": {"year": datetime.now().year + 1}}

    if "this year" in q_low:
        return {"intent": "revenue_forecast",
                "params": {"year": datetime.now().year}}



    # --------------- FALLBACK -------------------
    return {"intent": "fallback", "params": {}}


def build_sql_from_template(params: dict) -> str:
    """Map templates to SQL strings (keeps them safe and consistent)."""
    t = params.get("template")

    # Normalize IDs to uppercase
    meter_id = params.get("meter_id", "")
    if meter_id:
        meter_id = meter_id.upper()

    feeder_id = params.get("feeder_id", "")
    if feeder_id:
        feeder_id = feeder_id.upper()

    zone = params.get("zone", "")
    if zone:
        zone = zone.capitalize()

    # --- Templates ---
    if t == "meter_last_hours":
        return (
            "SELECT ts, load_kw FROM meter_readings "
            f"WHERE UPPER(meter_id)='{meter_id}' "
            f"AND ts >= DATETIME('now','-{params['hours']} hours') "
            "ORDER BY ts ASC"
        )

    if t == "meter_last_days":
        return (
            "SELECT ts, load_kw FROM meter_readings "
            f"WHERE UPPER(meter_id)='{meter_id}' "
            f"AND ts >= DATETIME('now','-{params['days']} days') "
            "ORDER BY ts ASC"
        )

    if t == "meter_latest":
        return (
            "SELECT ts, load_kw FROM meter_readings "
            f"WHERE UPPER(meter_id)='{meter_id}' "
            "ORDER BY ts DESC LIMIT 1"
        )

    if t == "top_n_peaks_month":
        return (
            "SELECT ts, load_kw FROM meter_readings "
            f"WHERE UPPER(meter_id)='{meter_id}' "
            "AND strftime('%Y-%m', ts)=strftime('%Y-%m','now') "
            f"ORDER BY load_kw DESC LIMIT {params['n']}"
        )

    if t == "zone_highest_load":
        period = params.get("period", "month")
        if period == "yesterday":
            cond = "DATE(r.ts) = DATE('now','-1 day')"
        elif period == "week":
            cond = "r.ts >= DATETIME('now','-7 days')"
        else:
            cond = "strftime('%Y-%m', r.ts)=strftime('%Y-%m','now')"

        return (
            "SELECT z.zone_name, MAX(r.load_kw) AS peak_load "
            "FROM meter_readings r "
            "JOIN meters m ON r.meter_id = m.meter_id "
            "JOIN feeders f ON m.feeder_id = f.feeder_id "
            "JOIN zones z ON f.zone_id = z.zone_id "
            f"WHERE {cond} "
            "GROUP BY z.zone_name "
            "ORDER BY peak_load DESC LIMIT 1"
        )

    if t == "zone_avg_week":
        return (
            "SELECT z.zone_name, AVG(r.load_kw) AS avg_load "
            "FROM meter_readings r "
            "JOIN meters m ON r.meter_id = m.meter_id "
            "JOIN feeders f ON m.feeder_id = f.feeder_id "
            "JOIN zones z ON f.zone_id = z.zone_id "
            f"WHERE z.zone_name = '{zone}' "
            "AND r.ts >= DATETIME('now','-7 days') "
            "GROUP BY z.zone_name"
        )

    if t == "feeder_highest_peak_yesterday":
        cond = "DATE(r.ts) = DATE('now','-1 day')"
        return (
            "SELECT r.ts, r.load_kw, f.feeder_id "
            "FROM meter_readings r "
            "JOIN meters m ON r.meter_id = m.meter_id "
            "JOIN feeders f ON m.feeder_id = f.feeder_id "
            f"WHERE {cond} "
            + (f"AND UPPER(f.feeder_id) = '{feeder_id}' " if feeder_id else "")
            + "ORDER BY r.load_kw DESC LIMIT 1"
        )

    if t == "zone_hourly_trend_days":
        return (
            "SELECT r.ts, r.load_kw, z.zone_name "
            "FROM meter_readings r "
            "JOIN meters m ON r.meter_id = m.meter_id "
            "JOIN feeders f ON m.feeder_id = f.feeder_id "
            "JOIN zones z ON f.zone_id = z.zone_id "
            f"WHERE z.zone_name = '{zone}' "
            f"AND r.ts >= DATETIME('now','-{params['days']} days') "
            "ORDER BY r.ts ASC"
        )

    if t == "meter_top_n_timestamps_month":
        return (
            "SELECT ts, load_kw FROM meter_readings "
            f"WHERE UPPER(meter_id) = '{meter_id}' "
            "AND strftime('%Y-%m', ts)=strftime('%Y-%m','now') "
            f"ORDER BY load_kw DESC LIMIT {params['n']}"
        )

    return ""


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FASTAPI APP SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

logging.info(f"üìÇ Using DB path: {DB_PATH}")
logging.info("‚úÖ Database file found" if os.path.exists(DB_PATH) else "‚ùå DB NOT FOUND")


def query_db(sql, params=()):
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    try:
        rows = conn.execute(sql, params).fetchall()
        return [dict(r) for r in rows]
    finally:
        conn.close()

def get_user_by_username(username: str):
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute(
        "SELECT username, password_hash, role FROM users WHERE username = ?",
        (username,)
    )
    row = cur.fetchone()
    conn.close()
    return row


@app.on_event("startup")
def startup_event():
    logging.info("‚ö° FASTAPI loaded ‚Äî forecasting + analytics + chatbot ready ‚ö°")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BASIC ENDPOINTS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

@app.get("/ping")
def ping():
    return {"message": "backend is running üöÄ"}


@app.post("/login")
def login(payload: dict = Body(...)):
    username = payload.get("username")
    password = payload.get("password")

    if not username or not password:
        raise HTTPException(status_code=400, detail="Missing username or password")

    user = get_user_by_username(username)
    if not user:
        raise HTTPException(status_code=401, detail="Invalid credentials")

    db_username, password_hash, role = user

    if not pwd_context.verify(password, password_hash):
        raise HTTPException(status_code=401, detail="Invalid credentials")

    return {
        "username": db_username,
        "role": role
    }


@app.get("/latest")
def latest(meter_id: str):
    sql = """
    SELECT * FROM meter_readings
    WHERE meter_id = ?
    ORDER BY ts DESC
    LIMIT 1
    """
    return query_db(sql, (meter_id,))


@app.get("/all")
def all_data(meter_id: str):
    sql = """
    SELECT * FROM meter_readings
    WHERE meter_id = ?
    ORDER BY ts ASC
    """
    return query_db(sql, (meter_id,))


@app.get("/range")
def range_data(meter_id: str, start: str, end: str):
    sql = """
    SELECT * FROM meter_readings
    WHERE meter_id = ?
    AND ts BETWEEN ? AND ?
    ORDER BY ts ASC
    """
    return query_db(sql, (meter_id, start, end))


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FORECAST ENDPOINT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
@app.get("/forecast")
def forecast(meter_id: str = "MTR001", hours: int = 24):
    logging.info("üî• Forecast endpoint called for meter_id=%s, hours=%s", meter_id, hours)

    model_path = os.path.join(BASE_DIR, "models", f"forecast_{meter_id}.pkl")
    if not os.path.exists(model_path):
        logging.error("Model not found: %s", model_path)
        return {"error": f"No trained model found for meter {meter_id}"}

    model_bundle = joblib.load(model_path)
    model = model_bundle["model"]
    feature_cols = model_bundle["feature_cols"]

    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        """
        SELECT ts, load_kw
        FROM meter_readings
        WHERE meter_id = ?
        ORDER BY ts ASC
        """,
        conn,
        params=(meter_id,),
    )
    conn.close()

    if df.empty:
        return {"error": f"No readings found for meter {meter_id}"}

    df["ts"] = pd.to_datetime(df["ts"])
    df = df.set_index("ts").asfreq("15T")
    df["load_kw"] = df["load_kw"].interpolate()

    history = df.copy()
    timestamps = []
    forecasts = []
    steps = int((hours * 60) / 15)

    for _ in range(steps):
        next_ts = history.index[-1] + timedelta(minutes=15)
        hour = next_ts.hour
        dow = next_ts.dayofweek
        sin_hour = np.sin(2 * np.pi * hour / 24)
        cos_hour = np.cos(2 * np.pi * hour / 24)
        lag_1 = history["load_kw"].iloc[-1]
        lag_4 = history["load_kw"].iloc[-4] if len(history) >= 4 else lag_1
        lag_96 = history["load_kw"].iloc[-96] if len(history) >= 96 else lag_1

        row = {
            "sin_hour": sin_hour,
            "cos_hour": cos_hour,
            "dow": dow,
            "lag_1": lag_1,
            "lag_4": lag_4,
            "lag_96": lag_96,
        }
        X_next = np.array([[row[c] for c in feature_cols]])
        y_pred = model.predict(X_next)[0]

        forecasts.append(float(y_pred))
        timestamps.append(str(next_ts))
        history.loc[next_ts, "load_kw"] = y_pred

    return {
        "meter_id": meter_id,
        "start": timestamps[0] if timestamps else None,
        "end": timestamps[-1] if timestamps else None,
        "forecast": [{"ts": t, "forecast_kw": f} for t, f in zip(timestamps, forecasts)],
    }


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ REVENUE FORECAST ENDPOINT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
@app.get("/revenue_forecast")
def revenue_forecast(year: int):
    """
    Predicts revenue for a given YEAR using the trained revenue model.
    """
    model_path = os.path.join(BASE_DIR, "models", "revenue_model.pkl")

    if not os.path.exists(model_path):
        return {"error": "Revenue model not trained. Run train_revenue_model.py first."}

    bundle = joblib.load(model_path)
    model = bundle["model"]

    prediction = model.predict([[year]])[0]

    return {
        "year": year,
        "predicted_revenue": float(prediction)
    }



@app.get("/revenue_range")
def revenue_range(start_year: int, end_year: int):
    model_path = os.path.join(BASE_DIR, "models", "revenue_model.pkl")

    if not os.path.exists(model_path):
        return {"error": "Revenue model not trained."}

    bundle = joblib.load(model_path)
    model = bundle["model"]

    values = []
    for y in range(start_year, end_year + 1):
        pred = float(model.predict([[y]])[0])
        values.append({"year": y, "revenue": pred})

    return {"start": start_year, "end": end_year, "values": values}




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ANALYTICS ENDPOINT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
@app.post("/analytics")
def analytics(payload: dict = Body(...)):
    task = payload.get("task")
    meter_id = payload.get("meter_id")
    zone = payload.get("zone")
    days = int(payload.get("days", 30))

    conn = sqlite3.connect(DB_PATH)
    df = pd.read_sql_query(
        """
        SELECT r.ts, r.load_kw, m.meter_id, z.zone_name,
               w.temperature_c, w.humidity_pct, w.solar_irradiance
        FROM meter_readings r
        JOIN meters m ON r.meter_id = m.meter_id
        JOIN feeders f ON m.feeder_id = f.feeder_id
        JOIN zones z ON f.zone_id = z.zone_id
        LEFT JOIN weather w ON z.zone_id = w.zone_id AND r.ts = w.ts
        """,
        conn,
    )
    conn.close()

    df["ts"] = pd.to_datetime(df["ts"])
    df = df[df["ts"] >= pd.Timestamp.now() - pd.Timedelta(days=days)]

    if zone:
        df = df[df["zone_name"] == zone]
    if meter_id:
        df = df[df["meter_id"] == meter_id]

    if df.empty:
        return {"message": "No data available", "chart_data": None}

    # Correlation: temperature
    if task == "correlation_weather_load":
        corr = float(df["load_kw"].corr(df["temperature_c"]))
        return {"message": f"Temperature ‚Üí Load correlation = {corr:.3f}", "chart_data": df[["ts", "load_kw"]].to_dict(orient="records")}

    # Correlation: humidity
    if task == "correlation_humidity_load":
        corr = float(df["load_kw"].corr(df["humidity_pct"]))
        return {"message": f"Humidity ‚Üí Load correlation = {corr:.3f}", "chart_data": df[["ts", "load_kw"]].to_dict(orient="records")}

    # Seasonal pattern
    if task == "seasonal_pattern":
        df["month"] = df["ts"].dt.month
        monthly = df.groupby("month")["load_kw"].mean().reset_index()
        monthly.columns = ["ts", "load_kw"]
        return {"message": "Seasonal trend generated", "chart_data": monthly.to_dict(orient="records")}

    # Billing estimation (simple)
    if task == "billing_estimation":
        total_kwh = df["load_kw"].sum()
        avg_rate = 8.5
        est = total_kwh * avg_rate
        return {"message": f"Estimated bill = ‚Çπ{est:.2f}", "chart_data": None}

    # Anomaly detection (z-score)
    if task == "anomaly_detection":
        results = []
        for m in df["meter_id"].unique():
            subset = df[df["meter_id"] == m]
            mean = subset["load_kw"].mean()
            std = subset["load_kw"].std()
            if std == 0 or pd.isna(std):
                continue
            z = abs((subset["load_kw"] - mean) / std)
            if (z > 2.5).any():
                results.append({"meter_id": m, "avg_load": round(mean, 2)})
        return {"message": f"{len(results)} anomalous meters detected", "details": results}
    
    # ZONE-LEVEL anomaly detection
    if task == "zone_anomaly_detection":
        results = []
        subset = df[df["zone_name"] == zone]
        if not subset.empty:
            mean = subset["load_kw"].mean()
            std = subset["load_kw"].std()
            if std > 0:
                subset["z"] = abs((subset["load_kw"] - mean) / std)
                anomalies = subset[subset["z"] > 2.5]
                results = anomalies[["ts", "load_kw"]].to_dict(orient="records")
        return {
            "message": f"Anomalies detected in {zone} zone (last {days} days)",
            "details": results
        }


    # Peak cause analysis
    if task == "peak_cause_analysis":
        threshold = df["load_kw"].quantile(0.90)
        peaks = df[df["load_kw"] >= threshold]
        corr_temp = peaks["load_kw"].corr(peaks["temperature_c"])
        corr_hum = peaks["load_kw"].corr(peaks["humidity_pct"])
        corr_solar = peaks["load_kw"].corr(peaks["solar_irradiance"])
        cause = max({"temperature": abs(corr_temp), "humidity": abs(corr_hum), "solar_irradiance": abs(corr_solar)},
                    key=lambda x: abs({"temperature": corr_temp, "humidity": corr_hum, "solar_irradiance": corr_solar}[x]))
        return {
            "message": f"Peak load most influenced by {cause}",
            "likelihood": {"temperature": round(float(corr_temp or 0), 3), "humidity": round(float(corr_hum or 0), 3), "solar_irradiance": round(float(corr_solar or 0), 3)},
            "top_peak_hours": peaks[["ts", "load_kw"]].to_dict(orient="records"),
        }
    
    # Revenue Forecast
    if task == "revenue_forecast":
        year = payload.get("year")
        if year is None:
            return {"error": "No year provided for revenue forecast"}

        # Load trained revenue model
        model_bundle = joblib.load(os.path.join(BASE_DIR, "models", "revenue_model.pkl"))
        model = model_bundle["model"]

        pred = model.predict([[year]])[0]

        return {
            "message": f"Estimated revenue for {year}: {pred:.2f}",
            "year": year,
            "predicted_revenue": float(pred)
        }



    return {"message": "Unknown analytics task", "chart_data": None}




# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ASK ENDPOINT (HYBRID ‚Äî RULES ‚Üí TEMPLATE ‚Üí RAG+LLM) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
@app.post("/ask")
def ask(payload: dict = Body(...)):
    logging.info("ASK endpoint triggered")

    question = (payload.get("question") or "").strip()
    if not question:
        return {"error": "No question provided"}

    # STEP 1 ‚Äî RULE-BASED INTENT DETECTION
    intent = detect_intent_and_params(question)
    logging.info(f"Intent detected: {intent}")

    # ------------------------------------------------------------------
    # 1) FORECAST INTENT
    # ------------------------------------------------------------------
    if intent["intent"] == "forecast":
        try:
            params = intent["params"]
            return {
                "task": "forecast",
                "meter_id": params["meter_id"],
                "hours": params["hours"],
                "forecast": forecast(
                    meter_id=params["meter_id"],
                    hours=params["hours"]
                ).get("forecast")
            }
        except Exception as e:
            return {"error": f"Forecast failed: {e}"}

    # ------------------------------------------------------------------
    # 2) ANALYTICS INTENT (frontend will call /analytics)
    # ------------------------------------------------------------------
    if intent["intent"] == "analytics":
        out = {"task": intent["task"]}
        out.update(intent.get("params", {}))
        return out
    
# ------------------- REVENUE FORECAST HANDLING -------------------

    if intent.get("intent") == "revenue_forecast":
        role = payload.get("role", "employee")  # default = employee
        require_admin(role)

        params = intent["params"]
        year = params.get("year")
        if not year:
            return {"error": "No year provided for revenue forecast"}

        try:
            model_obj = joblib.load(os.path.join(MODEL_DIR, "revenue_model.pkl"))
            model = model_obj.get("model", model_obj)

            pred = model.predict([[year]])[0]

            return {
                "intent": "revenue_forecast",
                "year": year,
                "value": float(pred)
            }
        except Exception as e:
            return {"error": f"Revenue model error: {str(e)}"}

        

    # ------------------- REVENUE RANGE FORECAST HANDLING -------------------

    if intent.get("intent") == "revenue_range_forecast":
        role = payload.get("role", "employee")
        require_admin(role)

        start_y = intent["params"]["start_year"]
        end_y = intent["params"]["end_year"]

        try:
            model_obj = joblib.load(os.path.join(MODEL_DIR, "revenue_model.pkl"))
            model = model_obj.get("model", model_obj)

            values = []
            for yr in range(start_y, end_y + 1):
                pred = float(model.predict([[yr]])[0])
                values.append({"year": yr, "revenue": pred})

            return {
                "intent": "revenue_range_forecast",
                "start_year": start_y,
                "end_year": end_y,
                "values": values
            }

        except Exception as e:
            return {"error": f"Revenue range forecast failed: {str(e)}"}


    # ------------------------------------------------------------------
    # 3) SQL TEMPLATE (safe)
    # ------------------------------------------------------------------
    if intent["intent"] == "sql_template":
        try:
            sql = build_sql_from_template(intent["params"])
            ok, msg = safe_sql_check(sql)
            if not ok:
                return {"error": msg, "generated_sql": sql}
            return {"sql": sql, "result": query_db(sql)}
        except Exception as e:
            logging.exception("Template SQL failed")
            return {"error": f"SQL template error: {e}"}
        


        

    # ------------------------------------------------------------------
    # 4) FALLBACK = RAG ‚Üí DeepSeek ‚Üí SQL or JSON
    # ------------------------------------------------------------------

    if retrieve_relevant_chunks is None:
        return {"error": "RAG unavailable"}

    # Retrieve embeddings context
    try:
        rag_chunks = retrieve_relevant_chunks(question, top_k=5)
    except Exception as e:
        return {"error": f"RAG retrieval failed: {e}"}

    # Prepare prompt for DeepSeek-R1
    system_prompt = ""
    try:
        with open(os.path.join(BASE_DIR, "prompt_sql_system.txt"), "r", encoding="utf-8") as f:
            system_prompt = f.read()
    except:
        logging.warning("System SQL prompt missing")

    rag_text = "\n\n".join(rag_chunks or [])

    full_prompt = f"""
{system_prompt}

### RELEVANT KNOWLEDGE ###
{rag_text}

### USER QUESTION ###
{question}

### RULES ###
Respond with ONE of the following (prioritize data queries if applicable):
1. A valid SQL query starting with SELECT (if the user asks for data)
2. A valid JSON object like {{"task": "...", ...}} (if specific analytics is needed)
3. A direct text response (if it's a general question or greeting)

NO reasoning, NO markdown around SQL/JSON, NO commentary if returning SQL/JSON.
If giving a text response, be brief and helpful.

### RESPONSE ###
"""

    # Run LLM
    try:
        proc = subprocess.run(
            LLM_RUN_CMD,
            input=full_prompt.encode("utf-8"),
            capture_output=True,
            timeout=120
        )
    except subprocess.TimeoutExpired:
        return {"error": "LLM call timed out"}
    except Exception as e:
        return {"error": f"LLM failed: {e}"}

    raw = proc.stdout.decode("utf-8", errors="ignore").strip()
    logging.info("DeepSeek raw output (first 600 chars): " + raw[:600])

    if not raw:
        return {"error": "LLM returned empty response"}

    # Remove reasoning <think> blocks
    cleaned = re.sub(r"<think>[\s\S]*?</think>", "", raw, flags=re.I).strip()

    # STEP A ‚Äî JSON DETECTION
    json_obj = extract_json_from_text(cleaned)
    if isinstance(json_obj, dict) and "task" in json_obj:
        return json_obj

    # ------------------------------------------------------------------
    # STEP B ‚Äî SQL DETECTION
    # ------------------------------------------------------------------

    # Remove markdown ```sql blocks
    sql_candidate = re.sub(r"```[\s\S]*?```", "", cleaned)
    # Remove ‚ÄúSQL:‚Äù or ‚ÄúQuery:‚Äù prefixes
    sql_candidate = re.sub(r"^(SQL|Query|Result)\s*:\s*", "", sql_candidate, flags=re.I)

    # Looking for SELECT
    sel = re.search(r"SELECT\b[\s\S]*", sql_candidate, flags=re.I)
    if sel:
        sql_final = sel.group(0).strip().rstrip(";")
        sql_final = auto_repair_sql(sql_final)

        # Safety check
        ok, msg = safe_sql_check(sql_final)
        if ok:
            try:
                result = query_db(sql_final)
                return {"sql": sql_final, "result": result}
            except Exception as e:
                logging.warning(f"SQL execution failed: {e}")
                # Fall through to conversational if SQL fails
        else:
            logging.warning(f"SQL safety check failed: {msg}")

    # ------------------------------------------------------------------
    # STEP C ‚Äî CONVERSATIONAL FALLBACK
    # ------------------------------------------------------------------
    # If it's not JSON and not valid SQL, return as plain content
    return {"content": cleaned}





# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CHAT ENDPOINT (VISUALIZATION READY) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
@app.post("/chat", response_model=ChatResponse)
def chat_endpoint(req: ChatRequest):
    """
    Wrapper around /ask and /analytics to provide a rich response.
    Returns strings, but if visualization is possible, the string will be
    a JSON-dumped object containing {"text": "...", "data": ...}
    """
    # 1. Get structured intent/result from /ask
    try:
        # Defaulting to admin role to allow full access for this demo
        ask_res = ask(payload={"question": req.message, "role": "admin"}) 
    except Exception as e:
        return ChatResponse(content=f"Error processing request: {str(e)}")

    # Helper to pack response
    def pack(text, data=None, type="text", extras=None):
        payload = {"text": text, "type": type}
        if data:
            payload["data"] = data
        if extras:
            payload.update(extras)
        return ChatResponse(content=json.dumps(payload))

    # Error handling
    if "error" in ask_res:
        return pack(f"I encountered an error: {ask_res['error']}", type="error")

    # Case A: SQL Result
    if "sql" in ask_res:
        rows = ask_res.get("result", [])
        if not rows:
            return pack(f"Executed SQL: {ask_res['sql']}\n\nNo results found.", type="info")
        
        return pack(
            text=f"Executed SQL: {ask_res['sql']}", 
            data=rows, 
            type="sql"
        )

    # Case B: Forecast Result
    if ask_res.get("task") == "forecast":
        meter = ask_res.get("meter_id")
        fc = ask_res.get("forecast", [])
        if not fc:
             return pack(f"Generated forecast for {meter}, but received no data points.", type="warning")
        
        start = fc[0].get("ts")
        end = fc[-1].get("ts")
        return pack(
            text=f"Forecast generated for {meter} from {start} to {end}.",
            data=fc,
            type="chart",
            extras={"chartType": "line", "xKey": "ts", "yKey": "forecast_kw", "yLabel": "Load (kW)"}
        )

    # Case C: Revenue Forecast
    if ask_res.get("intent") == "revenue_forecast":
        return pack(
            text=f"Predicted Revenue for {ask_res.get('year')}: ${ask_res.get('value', 0):,.2f}", 
            type="text"
        )

    if ask_res.get("intent") == "revenue_range_forecast":
        vals = ask_res.get("values", [])
        return pack(
            text=f"Revenue Forecast {ask_res.get('start_year')} - {ask_res.get('end_year')}",
            data=vals,
            type="chart",
            extras={"chartType": "line", "xKey": "year", "yKey": "revenue", "yLabel": "Revenue ($)"}
        )

    # Case D: Analytics
    if ask_res.get("task") in ["correlation_weather_load", "correlation_humidity_load", "seasonal_pattern", "billing_estimation", "anomaly_detection", "zone_anomaly_detection", "peak_cause_analysis", "revenue_forecast"]:
        # Execute analytics
        try:
            ana_res = analytics(payload=ask_res)
            msg = ana_res.get("message", "Analysis complete.")
            
            # Decide visualization based on what keys are present
            if "chart_data" in ana_res and ana_res["chart_data"]:
                return pack(
                    text=msg,
                    data=ana_res["chart_data"],
                    type="chart",
                    extras={"chartType": "line", "xKey": "ts", "yKey": "load_kw", "yLabel": "Load (kW)"}
                )
            
            if "details" in ana_res and ana_res["details"]:
                return pack(text=msg, data=ana_res["details"], type="table")
            
            return pack(text=msg, type="text")

        except Exception as e:
            return pack(f"Error executing analytics: {str(e)}", type="error")

    # Case E: Direct Message
    if "content" in ask_res:
        return pack(ask_res["content"])
    
    # Case F: Generic Message
    if "message" in ask_res:
        return pack(ask_res["message"])

    # Fallback
    return pack(f"Result: {json.dumps(ask_res, default=str)}")



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ENTRYPOINT ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
if __name__ == "__main__":
    import uvicorn
    logging.info("Starting uvicorn...")
    uvicorn.run(app, host="127.0.0.1", port=8001, log_level="info")

