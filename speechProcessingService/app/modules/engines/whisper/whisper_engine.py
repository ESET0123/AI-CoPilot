import os
import sys
from faster_whisper import WhisperModel
from app.core.logger import log_with_prefix

class WhisperEngine:
    _instance = None
    _model = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(WhisperEngine, cls).__new__(cls)
        return cls._instance

    def initialize(self, model_name: str = "large-v3", device: str = "cpu", compute_type: str = "int8"):
        if self._model is None:
            log_with_prefix("WhisperEngine", f"ЁЯЪА Loading Whisper model: {model_name} on {device}...")
            
            # Check for custom model path
            script_dir = os.path.dirname(os.path.abspath(__file__))
            potential_model_path = os.path.join(script_dir, "..", "..", "..", "models", f"whisper-{model_name}")
            
            if os.path.exists(potential_model_path):
                model_name = potential_model_path
                log_with_prefix("WhisperEngine", f"ЁЯУВ Using local model path: {model_name}")

            try:
                self._model = WhisperModel(model_name, device=device, compute_type=compute_type)
                log_with_prefix("WhisperEngine", "тЬЕ Model loaded successfully")
            except Exception as e:
                log_with_prefix("WhisperEngine", f"тЭМ Failed to load model: {str(e)}", level="error")
                raise e

    def transcribe(self, file_path: str, language: str = None, task: str = "transcribe"):
        if self._model is None:
            self.initialize()

        log_with_prefix("WhisperEngine", f"ЁЯОд Transcribing {os.path.basename(file_path)} (Lang: {language}, Task: {task})")
        
        params = {
            "beam_size": 5,
            "task": task,
            "condition_on_previous_text": False
        }
        
        if language and language != "auto":
            params["language"] = language
            
            # Adding priming prompts for Indian languages
            prompts = {
                "hi": "рдирдорд╕реНрддреЗ, рдЖрдк рдХреИрд╕реЗ рд╣реИрдВ? рдЖрдЬ рдореМрд╕рдо рдмрд╣реБрдд рдЕрдЪреНрдЫрд╛ рд╣реИред рдореБрдЭреЗ рд╣рд┐рдВрджреА рдореЗрдВ рдмрд╛рдд рдХрд░рдирд╛ рдкрд╕рдВрдж рд╣реИред",
                # "bn": "ржиржорж╕рзНржХрж╛рж░, ржЖржкржирж┐ ржХрзЗржоржи ржЖржЫрзЗржи? ржЖржЬ ржЖржмрж╣рж╛ржУржпрж╝рж╛ ржЦрзБржм ржнрж╛рж▓рзЛред ржЖржорж┐ ржмрж╛ржВрж▓рж╛ ржмрж▓рждрзЗ ржнрж╛рж▓рзЛржмрж╛рж╕рж┐ред",
                # "ta": "ро╡рогроХрпНроХроорпН, роирпАроЩрпНроХро│рпН роОрокрпНрокроЯро┐ роЗро░рпБроХрпНроХро┐ро▒рпАро░рпНроХро│рпН? родрооро┐ро┤рпН рооро┐роХро╡рпБроорпН роЕро┤роХро╛рой роорпКро┤ро┐.",
                # "te": "р░ир░ор░╕р▒Нр░Хр░╛р░░р░В, р░ор▒Ар░░р▒Б р░Ор░▓р░╛ р░Йр░ир▒Нр░ир░╛р░░р▒Б? р░др▒Жр░▓р▒Бр░Чр▒Б р░нр░╛р░╖ р░Ър░╛р░▓р░╛ р░мр░╛р░Чр▒Бр░Вр░Яр▒Бр░Вр░жр░┐.",
                # "mr": "рдирдорд╕реНрдХрд╛рд░, рддреБрдореНрд╣реА рдХрд╕реЗ рдЖрд╣рд╛рдд? рдорд░рд╛рдареА рдорд╛рдЭреА рдорд╛рддреГрднрд╛рд╖рд╛ рдЖрд╣реЗ.",
                # "gu": "ркиркорк╕рлНркдрлЗ, ркдркорлЗ ркХрлЗрко ркЫрлЛ? ркЧрлБркЬрк░рк╛ркдрлА ркорлАркарлА р░нр░╛р░╖ ркЫрлЗ.",
                # "kn": "р▓ир▓ор▓╕р│Нр▓Хр▓╛р▓░, р▓ир│Ар▓╡р│Б р▓╣р│Зр▓Чр▓┐р▓жр│Нр▓жр│Ар▓░р▓┐? р▓Хр▓ир│Нр▓ир▓б р▓ир▓╛р▓бр│Б р▓╕р│Бр▓Вр▓жр▓░р▓╡р▓╛р▓Чр▓┐р▓жр│Ж.",
                # "ml": "р┤ир┤ор┤╕р╡Нр┤Хр┤╛р┤░р┤В, р┤╕р╡БрзБржЦр┤ор┤╛р┤гр╡Л? р┤ор┤▓р┤пр┤╛р┤│р┤В р┤ор┤ир╡Лр┤╣р┤░р┤ор┤╛р┤п р░нр░╛р░╖р┤пр┤╛р┤гр╡Н.",
                # "pa": "ри╕ридри┐ ри╕рйНри░рйА риЕриХри╛ри▓, ридрйБри╕рйАриВ риХри┐ри╡рйЗриВ ри╣рйЛ? рикрй░риЬри╛римрйА римри╣рйБрид ри╡ризрйАриЖ ринри╛ри╕ри╝ри╛ ри╣рйИред",
            }
            if language in prompts:
                params["initial_prompt"] = prompts[language]

        segments, info = self._model.transcribe(file_path, **params)
        
        text = " ".join([segment.text for segment in segments]).strip()
        
        return {
            "text": text,
            "detected_language": info.language,
            "language_probability": info.language_probability
        }

# Global instance
whisper_engine = WhisperEngine()
